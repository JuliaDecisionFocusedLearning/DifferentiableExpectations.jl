var documenterSearchIndex = {"docs":
[{"location":"api/#API-reference","page":"API reference","title":"API reference","text":"","category":"section"},{"location":"api/#Public","page":"API reference","title":"Public","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"Modules = [DifferentiableExpectations]\nPrivate = false","category":"page"},{"location":"api/#DifferentiableExpectations.DifferentiableExpectations","page":"API reference","title":"DifferentiableExpectations.DifferentiableExpectations","text":"DifferentiableExpectations\n\nA Julia package for differentiating through expectations with Monte-Carlo estimates.\n\nExports\n\nDifferentiableExpectation\nFixedAtomsProbabilityDistribution\nReinforce\nReparametrization\n\n\n\n\n\n","category":"module"},{"location":"api/#DifferentiableExpectations.DifferentiableExpectation","page":"API reference","title":"DifferentiableExpectations.DifferentiableExpectation","text":"DifferentiableExpectation{threaded}\n\nAbstract supertype for differentiable parametric expectations F : Œ∏ -> ùîº[f(X)] where X ‚àº p(Œ∏), whose value and derivative are approximated with Monte-Carlo averages.\n\nType parameters\n\nthreaded::Bool: specifies whether the sampling should be performed in parallel\n\nRequired fields\n\ndist_constructor: The constructor of the probability distribution, such that calling dist_constructor(Œ∏...) generates an object corresponding to p(Œ∏). This object must satisfy:\nthe Random API\nthe DensityInterface.jl API\nf: The function applied inside the expectation.\nrng::AbstractRNG: The random number generator.\nnb_samples::Integer: The number of Monte-Carlo samples.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiableExpectations.DifferentiableExpectation-Union{Tuple, Tuple{threaded}} where threaded","page":"API reference","title":"DifferentiableExpectations.DifferentiableExpectation","text":"(F::DifferentiableExpectation)(Œ∏...; kwargs...)\n\nReturn a Monte-Carlo average (1/s) ‚àëf(x·µ¢) where the x·µ¢ ‚àº p(Œ∏) are iid samples.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiableExpectations.FixedAtomsProbabilityDistribution","page":"API reference","title":"DifferentiableExpectations.FixedAtomsProbabilityDistribution","text":"FixedAtomsProbabilityDistribution\n\nA probability distribution with finite support and fixed atoms.\n\nWhenever its expectation is differentiated, only the weights are considered active, whereas the atoms are considered constant.\n\nFields\n\natoms::Vector\nweights::Vector{W} where W<:Real\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiableExpectations.Reinforce","page":"API reference","title":"DifferentiableExpectations.Reinforce","text":"Reinforce{threaded} <: DifferentiableExpectation{threaded}\n\nDifferentiable parametric expectation F : Œ∏ -> ùîº[f(X)] where X ‚àº p(Œ∏) using the REINFORCE (or score function) gradient estimator:\n\n‚àÇF(Œ∏) = ùîº[f(X) ‚àá‚ÇÇlogp(X,Œ∏)·µÄ]\n\nConstructor\n\nReinforce(\n    f,\n    dist_constructor,\n    dist_gradlogpdf=nothing;\n    rng=Random.default_rng(),\n    nb_samples=1,\n    threaded=false\n)\n\nFields\n\nf::Any\ndist_constructor::Any\ndist_logdensity_grad::Any\nrng::Random.AbstractRNG\nnb_samples::Int64\n\nSee also\n\nDifferentiableExpectation\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiableExpectations.Reparametrization","page":"API reference","title":"DifferentiableExpectations.Reparametrization","text":"Reparametrization{threaded} <: DifferentiableExpectation{threaded}\n\nDifferentiable parametric expectation F : Œ∏ -> ùîº[f(X)] where X ‚àº p(Œ∏) using the reparametrization (or pathwise) gradient estimator: if X = g(Z,Œ∏) where Z ‚àº q then\n\n‚àÇF(Œ∏) = ùîº_q[‚àÇf(g(Z,Œ∏)) ‚àÇ‚ÇÇg(Z,Œ∏)·µÄ]\n\nConstructor\n\nReparametrization(\n    f,\n    dist_constructor,\n    rng=Random.default_rng(),\n    nb_samples=1,\n    threaded=false\n)\n\nFields\n\nf::Any\ndist_constructor::Any\nrng::Random.AbstractRNG\nnb_samples::Int64\n\nSee also\n\nDifferentiableExpectation\n\n\n\n\n\n","category":"type"},{"location":"api/#Private","page":"API reference","title":"Private","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"Modules = [DifferentiableExpectations]\nPublic = false","category":"page"},{"location":"api/#DifferentiableExpectations.presamples-Tuple{DifferentiableExpectation, Vararg{Any}}","page":"API reference","title":"DifferentiableExpectations.presamples","text":"presamples(F::DifferentiableExpectation, Œ∏...)\n\nReturn a vector [x‚ÇÅ, ..., x‚Çõ] or matrix [x‚ÇÅ ... x‚Çõ] where the x·µ¢ ‚àº p(Œ∏) are iid samples.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiableExpectations.samples-Union{Tuple{threaded}, Tuple{DifferentiableExpectation{threaded}, Vararg{Any}}} where threaded","page":"API reference","title":"DifferentiableExpectations.samples","text":"samples(F::DifferentiableExpectation, Œ∏...; kwargs...)\n\nReturn a vector [f(x‚ÇÅ), ..., f(x‚Çõ)] where the x·µ¢ ‚àº p(Œ∏) are iid samples.\n\n\n\n\n\n","category":"method"},{"location":"background/#Background","page":"Background","title":"Background","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"Consider a function f mathbbR^n to mathbbR^m and a parametric probability distribution p(theta) on the input space mathbbR^n. Given a random variable X sim p(theta), we want to differentiate the following expectation with respect to theta:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"F(theta) = mathbbE_p(theta)f(X)","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Since F is a vector-to-vector function, the key quantity we want to compute is its Jacobian matrix partial F(theta) in mathbbR^m times n. However, to implement automatic differentiation, we only need vector-Jacobian products (VJPs) partial F(theta)^top v with v in mathbbR^m, also called pullbacks. See the book by Blondel and Roulet (2024) to know more.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Most of the math below is taken from Mohamed et al. (2020).","category":"page"},{"location":"background/#REINFORCE","page":"Background","title":"REINFORCE","text":"","category":"section"},{"location":"background/#Basics","page":"Background","title":"Basics","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"The REINFORCE estimator is derived with the help of the identity nabla log u = nabla u  u:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginaligned\nF(theta + varepsilon)\n = int f(x)  p(x theta + varepsilon)  mathrmdx \n approx int f(x)  left(p(x theta) + nabla_theta p(x theta)^top varepsilonright)  mathrmdx \n = int f(x)  left(p(x theta) + p(x theta) nabla_theta log p(x theta)^top varepsilonright)  mathrmdx \n = F(theta) + left(int f(x)  p(x theta) nabla_theta log p(x theta)^top  mathrmdxright) varepsilon \n = F(theta) + mathbbE_p(theta) leftf(X) nabla_theta log p(X theta)^topright  varepsilon \nendaligned","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"We thus identify the Jacobian matrix:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"partial F(theta) = mathbbE_p(theta) leftf(X) nabla_theta log p(X theta)^topright","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"And the vector-Jacobian product:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"partial F(theta)^top v = mathbbE_p(theta) left(f(X)^top v) nabla_theta log p(X theta)right","category":"page"},{"location":"background/#Variance-reduction","page":"Background","title":"Variance reduction","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"warning: Warning\nWork in progress.","category":"page"},{"location":"background/#Reparametrization","page":"Background","title":"Reparametrization","text":"","category":"section"},{"location":"background/#Basics-2","page":"Background","title":"Basics","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"The reparametrization trick assumes that we can rewrite the random variable X sim p(theta) as X = g(Z theta), where Z sim q is another random variable whose distribution does not depend on theta.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginaligned\nF(theta + varepsilon)\n = int f(g(z theta + varepsilon))  q(z)  mathrmdz \n approx int fleft(g(z theta) + partial_theta g(z theta)  varepsilonright)  q(z)  mathrmdz \n approx F(theta) + int partial f(g(z theta))  partial_theta g(z theta)  varepsilon  q(z)  mathrmdz \n approx F(theta) + mathbbE_q left partial f(g(Z theta))  partial_theta g(Z theta) right  varepsilon \nendaligned","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"If we denote h(z theta) = f(g(z theta)), then we identify the Jacobian matrix:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"partial F(theta) = mathbbE_q left partial_theta h(Z theta) right","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"And the vector-Jacobian product:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"partial F(theta)^top v = mathbbE_q left partial_theta h(Z theta)^top v right","category":"page"},{"location":"background/#Catalogue","page":"Background","title":"Catalogue","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"The following reparametrizations are implemented:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Univariate Gaussian: X sim mathcalN(mu sigma^2) is equivalent to X = mu + sigma Z with Z sim mathcalN(0 1).","category":"page"},{"location":"background/#Bibliography","page":"Background","title":"Bibliography","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"Blondel,¬†M. and Roulet,¬†V. (2024). The Elements of Differentiable Programming, arXiv:2403.14606 [cs].\n\n\n\nMohamed,¬†S.; Rosca,¬†M.; Figurnov,¬†M. and Mnih,¬†A. (2020). Monte Carlo Gradient Estimation in Machine Learning. Journal¬†of¬†Machine¬†Learning¬†Research 21, 1‚Äì62.\n\n\n\n","category":"page"},{"location":"#DifferentiableExpectations.jl","page":"Home","title":"DifferentiableExpectations.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Dev) (Image: Build Status) (Image: Coverage) (Image: Code Style: Blue)","category":"page"},{"location":"","page":"Home","title":"Home","text":"A Julia package for differentiating through expectations with Monte-Carlo estimates.","category":"page"},{"location":"","page":"Home","title":"Home","text":"It allows the computation of approximate derivatives for functions of the form","category":"page"},{"location":"","page":"Home","title":"Home","text":"F(theta) = mathbbE_p(theta)f(X)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The following estimators are implemented:","category":"page"},{"location":"","page":"Home","title":"Home","text":"REINFORCE\nReparametrization","category":"page"}]
}
