var documenterSearchIndex = {"docs":
[{"location":"api/#API-reference","page":"API reference","title":"API reference","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"CollapsedDocStrings = true","category":"page"},{"location":"api/#Public","page":"API reference","title":"Public","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"Modules = [DifferentiableExpectations]\nPrivate = false","category":"page"},{"location":"api/#DifferentiableExpectations.DifferentiableExpectations","page":"API reference","title":"DifferentiableExpectations.DifferentiableExpectations","text":"DifferentiableExpectations\n\nA Julia package for differentiating through expectations with Monte-Carlo estimates.\n\nExports\n\nDifferentiableExpectation\nFixedAtomsProbabilityDistribution\nReinforce\nReparametrization\nempirical_distribution\n\n\n\n\n\n","category":"module"},{"location":"api/#DifferentiableExpectations.DifferentiableExpectation","page":"API reference","title":"DifferentiableExpectations.DifferentiableExpectation","text":"DifferentiableExpectation{t}\n\nAbstract supertype for differentiable parametric expectations E : Œ∏ -> ùîº[f(X)] where X ‚àº p(Œ∏), whose value and derivative are approximated with Monte-Carlo averages.\n\nSubtypes\n\nReinforce\nReparametrization\n\nCalling behavior\n\n(E::DifferentiableExpectation)(Œ∏...; kwargs...)\n\nReturn a Monte-Carlo average (1/S) ‚àëf(x·µ¢) where the x·µ¢ ‚àº p(Œ∏) are iid samples.\n\nType parameters\n\nthreaded::Bool: specifies whether the sampling should be performed in parallel\n\nRequired fields\n\nf: The function applied inside the expectation.\ndist_constructor: The constructor of the probability distribution.\nrng::AbstractRNG: The random number generator.\nnb_samples::Integer: The number of Monte-Carlo samples.\nseed::Union{Nothing,Integer}: The seed for the random number generator, reset before each call. Set to nothing for no seeding.\n\nThe field dist_constructor must be a callable such that dist_constructor(Œ∏...) generates an object dist that corresponds to p(Œ∏). The resulting object dist needs to satisfy:\n\nthe Random API for sampling with rand(rng, dist)\nthe DensityInterface.jl API for loglikelihoods with logdensityof(dist, x)\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiableExpectations.FixedAtomsProbabilityDistribution","page":"API reference","title":"DifferentiableExpectations.FixedAtomsProbabilityDistribution","text":"FixedAtomsProbabilityDistribution{threaded}\n\nA probability distribution with finite support and fixed atoms.\n\nWhenever its expectation is differentiated, only the weights are considered active, whereas the atoms are considered constant.\n\nExample\n\njulia> using DifferentiableExpectations, Statistics, Zygote\n\njulia> using DifferentiableExpectations: atoms, weights\n\njulia> dist = FixedAtomsProbabilityDistribution([2, 3], [0.4, 0.6]);\n\njulia> atoms(map(abs2, dist))\n2-element Vector{Int64}:\n 4\n 9\n\njulia> weights(map(abs2, dist))\n2-element Vector{Float64}:\n 0.4\n 0.6\n\njulia> mean(abs2, dist)\n7.0\n\njulia> gradient(mean, abs2, dist)[2]\n(atoms = nothing, weights = [4.0, 9.0])\n\nConstructor\n\nFixedAtomsProbabilityDistribution(\n    atoms::AbstractVector,\n    weights::AbstractVector=uniform_weights(atoms);\n    threaded=false\n)\n\nFields\n\natoms::AbstractVector\nweights::AbstractVector{<:Real}\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiableExpectations.Reinforce","page":"API reference","title":"DifferentiableExpectations.Reinforce","text":"Reinforce{threaded} <: DifferentiableExpectation{threaded}\n\nDifferentiable parametric expectation F : Œ∏ -> ùîº[f(X)] where X ‚àº p(Œ∏) using the REINFORCE (or score function) gradient estimator:\n\n‚àÇF(Œ∏) = ùîº[f(X) ‚àá‚ÇÇlogp(X,Œ∏)·µÄ]\n\nExample\n\nusing DifferentiableExpectations, Distributions, Zygote\n\nE = Reinforce(exp, Normal; nb_samples=10^5)\nE_true(Œº, œÉ) = mean(LogNormal(Œº, œÉ))\n\nŒº, œÉ = 0.5, 1,0\n‚àáE, ‚àáE_true = gradient(E, Œº, œÉ), gradient(E_true, Œº, œÉ)\nisapprox(collect(‚àáE), collect(‚àáE_true); rtol=1e-1)\n\n# output\n\ntrue\n\nConstructor\n\nReinforce(\n    f,\n    dist_constructor,\n    dist_logdensity_grad=nothing;\n    rng=Random.default_rng(),\n    nb_samples=1,\n    threaded=false,\n    seed=nothing\n)\n\nFields\n\nf::Any: function applied inside the expectation\ndist_constructor::Any: constructor of the probability distribution (Œ∏...) -> p(Œ∏)\ndist_logdensity_grad::Any: either nothing or a parameter gradient callable (x, Œ∏...) -> ‚àá‚ÇÇlogp(x, Œ∏)\nrng::Random.AbstractRNG: random number generator\nnb_samples::Int64: number of Monte-Carlo samples\nseed::Union{Nothing, Int64}: seed for the random number generator, reset before each call. Set to nothing for no seeding.\n\nSee also\n\nDifferentiableExpectation\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiableExpectations.Reparametrization","page":"API reference","title":"DifferentiableExpectations.Reparametrization","text":"Reparametrization{threaded} <: DifferentiableExpectation{threaded}\n\nDifferentiable parametric expectation F : Œ∏ -> ùîº[f(X)] where X ‚àº p(Œ∏) using the reparametrization (or pathwise) gradient estimator: if X = g(Z,Œ∏) where Z ‚àº q then\n\n‚àÇF(Œ∏) = ùîº_q[‚àÇf(g(Z,Œ∏)) ‚àÇ‚ÇÇg(Z,Œ∏)·µÄ]\n\nExample\n\nusing DifferentiableExpectations, Distributions, Zygote\n\nE = Reparametrization(exp, Normal; nb_samples=10^4)\nE_true(Œº, œÉ) = mean(LogNormal(Œº, œÉ))\n\nŒº, œÉ = 0.5, 1,0\n‚àáE, ‚àáE_true = gradient(E, Œº, œÉ), gradient(E_true, Œº, œÉ)\nisapprox(collect(‚àáE), collect(‚àáE_true); rtol=1e-1)\n\n# output\n\ntrue\n\nConstructor\n\nReparametrization(\n    f,\n    dist_constructor,\n    rng=Random.default_rng(),\n    nb_samples=1,\n    threaded=false,\n    seed=nothing\n)\n\nFields\n\nf::Any: function applied inside the expectation\ndist_constructor::Any: constructor of the probability distribution (Œ∏...) -> p(Œ∏)\nrng::Random.AbstractRNG: random number generator\nnb_samples::Int64: number of Monte-Carlo samples\nseed::Union{Nothing, Int64}: seed for the random number generator, reset before each call. Set to nothing for no seeding.\n\nSee also\n\nDifferentiableExpectation\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiableExpectations.empirical_distribution-Tuple{DifferentiableExpectation, Vararg{Any}}","page":"API reference","title":"DifferentiableExpectations.empirical_distribution","text":"empirical_distribution(E::DifferentiableExpectation, Œ∏...; kwargs...)\n\nReturn a uniform FixedAtomsProbabilityDistribution over {f(x‚ÇÅ), ..., f(x‚Çõ)}, where the x·µ¢ ‚àº p(Œ∏) are iid samples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Private","page":"API reference","title":"Private","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"Modules = [DifferentiableExpectations]\nPublic = false","category":"page"},{"location":"api/#DifferentiableExpectations.FixKwargs","page":"API reference","title":"DifferentiableExpectations.FixKwargs","text":"FixKwargs(f, kwargs)\n\nCallable struct that fixes the keyword arguments of f to kwargs..., and only accepts positional arguments.\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiableExpectations.TransformedDistribution","page":"API reference","title":"DifferentiableExpectations.TransformedDistribution","text":"TransformedDistribution\n\nRepresent the probability distribution p of a random variable X ‚àº p with a transformation X = T(Z) where Z ‚àº q.\n\nFields\n\nbase_dist::Any: the distribution q that gets transformed into p\ntransformation::Any: the transformation function T\n\n\n\n\n\n","category":"type"},{"location":"api/#Base.map-Tuple{Any, FixedAtomsProbabilityDistribution}","page":"API reference","title":"Base.map","text":"map(f, dist::FixedAtomsProbabilityDistribution)\n\nApply f to the atoms of dist, leave the weights unchanged.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, DifferentiableExpectations.TransformedDistribution}","page":"API reference","title":"Base.rand","text":"rand(rng, dist::TransformedDistribution)\n\nSample from dist by applying dist.transformation to dist.base_dist.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, FixedAtomsProbabilityDistribution}","page":"API reference","title":"Base.rand","text":"rand(rng, dist::FixedAtomsProbabilityDistribution)\n\nSample from the atoms of dist with probability proportional to their weights.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiableExpectations.atoms-Tuple{FixedAtomsProbabilityDistribution}","page":"API reference","title":"DifferentiableExpectations.atoms","text":"atoms(dist::FixedAtomsProbabilityDistribution)\n\nGet the vector of atoms of a distribution.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiableExpectations.empirical_predistribution-Tuple{DifferentiableExpectation, Vararg{Any}}","page":"API reference","title":"DifferentiableExpectations.empirical_predistribution","text":"empirical_predistribution(E::DifferentiableExpectation, Œ∏...)\n\nReturn a uniform FixedAtomsProbabilityDistribution over {x‚ÇÅ, ..., x‚Çõ}, where the x·µ¢ ‚àº p(Œ∏) are iid samples.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiableExpectations.maybe_eachcol-Tuple{AbstractMatrix}","page":"API reference","title":"DifferentiableExpectations.maybe_eachcol","text":"maybe_eachcol(x::AbstractMatrix)\n\nReturn eachcol(x).\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiableExpectations.maybe_eachcol-Tuple{AbstractVector}","page":"API reference","title":"DifferentiableExpectations.maybe_eachcol","text":"maybe_eachcol(x::AbstractVector)\n\nReturn x.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiableExpectations.mymap-Tuple{Val{true}, Vararg{Any}}","page":"API reference","title":"DifferentiableExpectations.mymap","text":"mymap(::Val{threaded}, args...)\n\nApply either tmap(args...) or map(args...) depending on the value of threaded.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiableExpectations.mymapreduce-Tuple{Val{true}, Vararg{Any}}","page":"API reference","title":"DifferentiableExpectations.mymapreduce","text":"mymapreduce(::Val{threaded}, args...)\n\nApply either tmapreduce(args...) or mapreduce(args...) depending on the value of threaded.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiableExpectations.reparametrize","page":"API reference","title":"DifferentiableExpectations.reparametrize","text":"reparametrize(dist)\n\nTurn a probability distribution p into a TransformedDistribution (q, T) such that the new distribution q does not depend on the parameters of p. These parameters are encoded (closed over) in the transformation function T.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiableExpectations.weights-Tuple{FixedAtomsProbabilityDistribution}","page":"API reference","title":"DifferentiableExpectations.weights","text":"weights(dist::FixedAtomsProbabilityDistribution)\n\nGet the vector of weights of a distribution.\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.mean-Tuple{Any, FixedAtomsProbabilityDistribution}","page":"API reference","title":"Statistics.mean","text":"mean(f, dist::FixedAtomsProbabilityDistribution)\n\nShortcut for mean(map(f, dist)).\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.mean-Tuple{FixedAtomsProbabilityDistribution}","page":"API reference","title":"Statistics.mean","text":"mean(dist::FixedAtomsProbabilityDistribution)\n\nCompute the expectation of dist, i.e. the sum of all atoms multiplied by their respective weights.\n\n\n\n\n\n","category":"method"},{"location":"background/#Background","page":"Background","title":"Background","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"Most of the math below is taken from Mohamed et al. (2020).","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Consider a function f mathbbR^n to mathbbR^m, a parameter theta in mathbbR^d and a parametric probability distribution p(theta) on the input space. Given a random variable X sim p(theta), we want to differentiate the expectation of Y = f(X) with respect to theta:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"E(theta) = mathbbEf(X) = int f(x)  p(x  theta) mathrmd x = int y  q(y  theta) mathrmd y","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Usually this is approximated with Monte-Carlo sampling: let x_1 dots x_S sim p(theta) be i.i.d., we have the estimator","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"E(theta) simeq frac1S sum_s=1^S f(x_s)","category":"page"},{"location":"background/#Autodiff","page":"Background","title":"Autodiff","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"Since E is a vector-to-vector function, the key quantity we want to compute is its Jacobian matrix partial E(theta) in mathbbR^m times n:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"partial E(theta) = int f(x)  nabla_theta p(x  theta)^top mathrmd x = int y  nabla_theta q(y  theta)^top  mathrmd y","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"However, to implement automatic differentiation, we only need the vector-Jacobian product (VJP) partial E(theta)^top bary with an output cotangent bary in mathbbR^m. See the book by Blondel and Roulet (Mar 2024) to know more.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Our goal is to rephrase this VJP as an expectation, so that we may approximate it with Monte-Carlo sampling as well.","category":"page"},{"location":"background/#REINFORCE","page":"Background","title":"REINFORCE","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"Implemented by Reinforce.","category":"page"},{"location":"background/#Score-function","page":"Background","title":"Score function","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"The REINFORCE estimator is derived with the help of the identity nabla log u = nabla u  u:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginaligned\npartial E(theta)\n = int f(x)  nabla_theta p(x  theta)^top  mathrmdx \n = int f(x)  nabla_theta log p(x  theta)^top p(x  theta)  mathrmdx \n = mathbbE leftf(X) nabla_theta log p(X  theta)^topright \nendaligned","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"And the VJP:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"partial E(theta)^top bary = mathbbE leftf(X)^top bary nabla_theta log p(X  theta)right","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Our Monte-Carlo approximation will therefore be:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"partial E(theta)^top bary simeq frac1S sum_s=1^S f(x_s)^top bary  nabla_theta log p(x_s  theta)","category":"page"},{"location":"background/#Variance-reduction","page":"Background","title":"Variance reduction","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"The REINFORCE estimator has high variance, but its variance is reduced by subtracting a so-called baseline b = frac1S sum_s=1^S f(x_s) (Kool et al., 2022).","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"For S  1 Monte-Carlo samples, we have","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginaligned\npartial E(theta)^top bary \n simeq frac1S sum_s=1^S left(f(x_s) - frac1S - 1sum_jneq s f(x_j) right)^top bary  nabla_thetalog p(x_s  theta)\n = frac1S - 1sum_s=1^S (f(x_s) - b)^top bary  nabla_thetalog p(x_s  theta)\nendaligned","category":"page"},{"location":"background/#Reparametrization","page":"Background","title":"Reparametrization","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"Implemented by Reparametrization.","category":"page"},{"location":"background/#Trick","page":"Background","title":"Trick","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"The reparametrization trick assumes that we can rewrite the random variable X sim p(theta) as X = g_theta(Z), where Z sim r is another random variable whose distribution r does not depend on theta.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"The expectation is rewritten with h = f circ g:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"E(theta) = mathbbEleft f(g_theta(Z)) right = mathbbEleft h_theta(Z) right","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"And we can directly differentiate through the expectation:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"partial E(theta) = mathbbE left partial_theta h_theta(Z) right","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"This yields the VJP:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"partial E(theta)^top bary = mathbbE left partial_theta h_theta(Z)^top bary right","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"We can use a Monte-Carlo approximation with i.i.d. samples z_1 dots z_S sim r:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"partial E(theta)^top bary simeq frac1S sum_s=1^S partial_theta h_theta(z_s)^top bary","category":"page"},{"location":"background/#Catalogue","page":"Background","title":"Catalogue","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"The following reparametrizations are implemented:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Univariate Normal: X sim mathcalN(mu sigma^2) is equivalent to X = mu + sigma Z with Z sim mathcalN(0 1).\nMultivariate Normal: X sim mathcalN(mu Sigma) is equivalent to X = mu + L Z with Z sim mathcalN(0 I) and L L^top = Sigma. The matrix L can be obtained by Cholesky decomposition of Sigma.","category":"page"},{"location":"background/#Probability-gradients","page":"Background","title":"Probability gradients","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"In the case where f is a function that takes values in a finite set mathcalY = y_1 cdots y_K, we may also want to compute the jacobian of the probability weights vector:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"q  theta longmapsto beginpmatrix q(y_1theta) = mathbbP(f(X) = y_1theta)  dots  q(y_Ktheta) = mathbbP(f(X) = y_Ktheta) endpmatrix","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"whose Jacobian is given by","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"partial_theta q(theta) = beginpmatrix nabla_theta q(y_1theta)^top  dots  nabla_theta q(y_Ktheta)^top endpmatrix","category":"page"},{"location":"background/#REINFORCE-probability-gradients","page":"Background","title":"REINFORCE probability gradients","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"The REINFORCE technique can be applied in a similar way:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"q(y_k  theta) = mathbbEmathbf1f(X) = y_k  = int mathbf1 f(x) = y_k  p(x  theta)  mathrmdx","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Differentiating through the integral,","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginaligned\nnabla_theta q(y_k  theta)\n = int mathbf1 f(x) = y_k  nabla_theta p(x  theta)  mathrmdx \n = mathbbE mathbf1 f(X) = y_k  nabla_theta log p(X  theta)\nendaligned","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"The Monte-Carlo approximation for this is","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"nabla_theta q(y_k  theta) simeq frac1S sum_s=1^S mathbf1 f(x_s) = y_k  nabla_theta log p(x_s  theta)","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"The VJP is then","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginaligned\npartial_theta q(theta)^top barq = sum_k=1^K barq_k nabla_theta q(y_k  theta)\nsimeq  frac1S sum_s=1^S leftsum_k=1^K barq_k mathbf1 f(x_s) = y_kright  nabla_theta log p(x_s  theta)\nendaligned","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"In our implementation, the empirical_distribution method outputs an empirical FixedAtomsProbabilityDistribution with uniform weights frac1S, where some x_s can be repeated.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"q  theta longmapsto beginpmatrix q(f(x_1)theta)  dots  q(f(x_S)  theta) endpmatrix","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"We therefore define the corresponding VJP as","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"partial_theta q(theta)^top barq = frac1S sum_s=1^S barq_s nabla_theta log p(x_s  theta)","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"If bar q comes from mean, we have bar q_s = f(x_s)^top bar y and we obtain the REINFORCE VJP.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"This VJP can be interpreted as an empirical expectation, to which we can also apply variance reduction: partial_theta q(theta)^top bar q approx frac1S-1sum_s(bar q_s - b) nabla_theta log p(x_stheta) with b = frac1Ssum_s bar q_s.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Again, if bar q comes from mean, we have bar q_s = f(x_s)^top bar y and b = b^top bar y. We then obtain the REINFORCE backward rule with variance reduction: partial_theta q(theta)^top bar q approx frac1S-1sum_s(f(x_s) - b)^top bar y nabla_theta log p(x_stheta)","category":"page"},{"location":"background/#Reparametrization-probability-gradients","page":"Background","title":"Reparametrization probability gradients","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"To leverage reparametrization, we perform a change of variables:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"q(y  theta) = mathbbEmathbf1h_theta(Z) = y  = int mathbf1 h_theta(z) = y  r(z)  mathrmdz","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Assuming that h_theta is invertible, we take z = h_theta^-1(u) and","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"mathrmdz = partial h_theta^-1(u)  mathrmdu","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"so that","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"q(y  theta) = int mathbf1 u = y  r(h_theta^-1(u))  partial h_theta^-1(u)  mathrmdu","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"We can now differentiate, but it gets tedious.","category":"page"},{"location":"background/#Bibliography","page":"Background","title":"Bibliography","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"Blondel,¬†M. and Roulet,¬†V. (Mar 2024). The Elements of Differentiable Programming, arXiv:2403.14606 [cs]. Accessed on Mar 22, 2024.\n\n\n\nKool,¬†W.; van¬†Hoof,¬†H. and Welling,¬†M. (2022). Buy 4 REINFORCE Samples, Get a Baseline for Free! ICLR. Accessed on Apr 17, 2023.\n\n\n\nMohamed,¬†S.; Rosca,¬†M.; Figurnov,¬†M. and Mnih,¬†A. (2020). Monte Carlo Gradient Estimation in Machine Learning. Journal¬†of¬†Machine¬†Learning¬†Research 21, 1‚Äì62. Accessed on Oct 21, 2022.\n\n\n\n","category":"page"},{"location":"#DifferentiableExpectations.jl","page":"Home","title":"DifferentiableExpectations.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Stable) (Image: Dev) (Image: Build Status) (Image: Coverage) (Image: Code Style: Blue)","category":"page"},{"location":"","page":"Home","title":"Home","text":"A Julia package for differentiating through expectations with Monte-Carlo estimates.","category":"page"},{"location":"","page":"Home","title":"Home","text":"It allows the computation of approximate derivatives for functions of the form","category":"page"},{"location":"","page":"Home","title":"Home","text":"F(theta) = mathbbE_p(theta)f(X)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The following estimators are implemented:","category":"page"},{"location":"","page":"Home","title":"Home","text":"REINFORCE\nReparametrization","category":"page"},{"location":"","page":"Home","title":"Home","text":"Warning: this package is experimental, use at your own risk and expect frequent breaking releases.","category":"page"}]
}
