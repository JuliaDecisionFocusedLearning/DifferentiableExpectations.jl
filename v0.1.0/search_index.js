var documenterSearchIndex = {"docs":
[{"location":"api/#API-reference","page":"API reference","title":"API reference","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"CollapsedDocStrings = true","category":"page"},{"location":"api/#Public","page":"API reference","title":"Public","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"Modules = [DifferentiableExpectations]\nPrivate = false","category":"page"},{"location":"api/#DifferentiableExpectations.DifferentiableExpectations","page":"API reference","title":"DifferentiableExpectations.DifferentiableExpectations","text":"DifferentiableExpectations\n\nA Julia package for differentiating through expectations with Monte-Carlo estimates.\n\nExports\n\nDifferentiableExpectation\nFixedAtomsProbabilityDistribution\nReinforce\nReparametrization\n\n\n\n\n\n","category":"module"},{"location":"api/#DifferentiableExpectations.DifferentiableExpectation","page":"API reference","title":"DifferentiableExpectations.DifferentiableExpectation","text":"DifferentiableExpectation{threaded}\n\nAbstract supertype for differentiable parametric expectations F : Œ∏ -> ùîº[f(X)] where X ‚àº p(Œ∏), whose value and derivative are approximated with Monte-Carlo averages.\n\nSubtypes\n\nReinforce\nReparametrization\n\nCalling behavior\n\n(F::DifferentiableExpectation)(Œ∏...; kwargs...)\n\nReturn a Monte-Carlo average (1/s) ‚àëf(x·µ¢) where the x·µ¢ ‚àº p(Œ∏) are iid samples.\n\nType parameters\n\nthreaded::Bool: specifies whether the sampling should be performed in parallel\n\nRequired fields\n\nf: The function applied inside the expectation.\ndist_constructor: The constructor of the probability distribution.\nrng::AbstractRNG: The random number generator.\nnb_samples::Integer: The number of Monte-Carlo samples.\n\nThe field dist_constructor must be a callable such that dist_constructor(Œ∏...) generates an object dist that corresponds to p(Œ∏). The resulting object dist needs to satisfy:\n\nthe Random API for sampling with rand(rng, dist)\nthe DensityInterface.jl API for loglikelihoods with logdensityof(dist, x)\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiableExpectations.FixedAtomsProbabilityDistribution","page":"API reference","title":"DifferentiableExpectations.FixedAtomsProbabilityDistribution","text":"FixedAtomsProbabilityDistribution{threaded}\n\nA probability distribution with finite support and fixed atoms.\n\nWhenever its expectation is differentiated, only the weights are considered active, whereas the atoms are considered constant.\n\nExample\n\njulia> using DifferentiableExpectations, Statistics, Zygote\n\njulia> dist = FixedAtomsProbabilityDistribution([2, 3], [0.4, 0.6]);\n\njulia> map(abs2, dist)\nFixedAtomsProbabilityDistribution{false}([4, 9], [0.4, 0.6])\n\njulia> mean(abs2, dist)\n7.0\n\njulia> gradient(mean, abs2, dist)[2]\n(atoms = nothing, weights = [4.0, 9.0])\n\nConstructor\n\nFixedAtomsProbabilityDistribution(\n    atoms::Vector,\n    weights::Vector;\n    threaded=false\n)\n\nFields\n\natoms::Vector\nweights::Vector{W} where W<:Real\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiableExpectations.Reinforce","page":"API reference","title":"DifferentiableExpectations.Reinforce","text":"Reinforce{threaded} <: DifferentiableExpectation{threaded}\n\nDifferentiable parametric expectation F : Œ∏ -> ùîº[f(X)] where X ‚àº p(Œ∏) using the REINFORCE (or score function) gradient estimator:\n\n‚àÇF(Œ∏) = ùîº[f(X) ‚àá‚ÇÇlogp(X,Œ∏)·µÄ]\n\nExample\n\nusing DifferentiableExpectations, Distributions, Zygote\n\nF = Reinforce(exp, Normal; nb_samples=10^5)\nF_true(Œº, œÉ) = mean(LogNormal(Œº, œÉ))\n\nŒº, œÉ = 0.5, 1,0\n‚àáF, ‚àáF_true = gradient(F, Œº, œÉ), gradient(F_true, Œº, œÉ)\nisapprox(collect(‚àáF), collect(‚àáF_true); rtol=1e-1)\n\n# output\n\ntrue\n\nConstructor\n\nReinforce(\n    f,\n    dist_constructor,\n    dist_logdensity_grad=nothing;\n    rng=Random.default_rng(),\n    nb_samples=1,\n    threaded=false\n)\n\nFields\n\nf::Any: function applied inside the expectation\ndist_constructor::Any: constructor of the probability distribution (Œ∏...) -> p(Œ∏)\ndist_logdensity_grad::Any: either nothing or a parameter gradient callable (x, Œ∏...) -> ‚àá‚ÇÇlogp(x, Œ∏)\nrng::Random.AbstractRNG: random number generator\nnb_samples::Int64: number of Monte-Carlo samples\n\nSee also\n\nDifferentiableExpectation\n\n\n\n\n\n","category":"type"},{"location":"api/#DifferentiableExpectations.Reparametrization","page":"API reference","title":"DifferentiableExpectations.Reparametrization","text":"Reparametrization{threaded} <: DifferentiableExpectation{threaded}\n\nDifferentiable parametric expectation F : Œ∏ -> ùîº[f(X)] where X ‚àº p(Œ∏) using the reparametrization (or pathwise) gradient estimator: if X = g(Z,Œ∏) where Z ‚àº q then\n\n‚àÇF(Œ∏) = ùîº_q[‚àÇf(g(Z,Œ∏)) ‚àÇ‚ÇÇg(Z,Œ∏)·µÄ]\n\nExample\n\nusing DifferentiableExpectations, Distributions, Zygote\n\nF = Reparametrization(exp, Normal; nb_samples=10^4)\nF_true(Œº, œÉ) = mean(LogNormal(Œº, œÉ))\n\nŒº, œÉ = 0.5, 1,0\n‚àáF, ‚àáF_true = gradient(F, Œº, œÉ), gradient(F_true, Œº, œÉ)\nisapprox(collect(‚àáF), collect(‚àáF_true); rtol=1e-1)\n\n# output\n\ntrue\n\nConstructor\n\nReparametrization(\n    f,\n    dist_constructor,\n    rng=Random.default_rng(),\n    nb_samples=1,\n    threaded=false\n)\n\nFields\n\nf::Any: function applied inside the expectation\ndist_constructor::Any: constructor of the probability distribution (Œ∏...) -> p(Œ∏)\nrng::Random.AbstractRNG: random number generator\nnb_samples::Int64: number of Monte-Carlo samples\n\nSee also\n\nDifferentiableExpectation\n\n\n\n\n\n","category":"type"},{"location":"api/#Private","page":"API reference","title":"Private","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"Modules = [DifferentiableExpectations]\nPublic = false","category":"page"},{"location":"api/#DifferentiableExpectations.TransformedDistribution","page":"API reference","title":"DifferentiableExpectations.TransformedDistribution","text":"TransformedDistribution\n\nRepresent the probability distribution p of a random variable X ‚àº p with a transformation X = T(Z) where Z ‚àº q.\n\nFields\n\nbase_dist::Any: the distribution q that gets transformed into p\ntransformation::Any: the transformation function T\n\n\n\n\n\n","category":"type"},{"location":"api/#Base.map-Union{Tuple{threaded}, Tuple{Any, FixedAtomsProbabilityDistribution{threaded}}} where threaded","page":"API reference","title":"Base.map","text":"map(f, dist::FixedAtomsProbabilityDistribution)\n\nApply f to the atoms of dist, leave the weights unchanged.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, DifferentiableExpectations.TransformedDistribution}","page":"API reference","title":"Base.rand","text":"rand(rng, dist::TransformedDistribution)\n\nSample from dist by applying dist.transformation to dist.base_dist.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, FixedAtomsProbabilityDistribution}","page":"API reference","title":"Base.rand","text":"rand(rng, dist::FixedAtomsProbabilityDistribution)\n\nSample from the atoms of dist with probability proportional to their weights.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiableExpectations.presamples-Tuple{DifferentiableExpectation, Vararg{Any}}","page":"API reference","title":"DifferentiableExpectations.presamples","text":"presamples(F::DifferentiableExpectation, Œ∏...)\n\nReturn a vector [x‚ÇÅ, ..., x‚Çõ] or matrix [x‚ÇÅ ... x‚Çõ] where the x·µ¢ ‚àº p(Œ∏) are iid samples.\n\n\n\n\n\n","category":"method"},{"location":"api/#DifferentiableExpectations.reparametrize","page":"API reference","title":"DifferentiableExpectations.reparametrize","text":"reparametrize(dist)\n\nTurn a probability distribution p into a TransformedDistribution (q, T) such that the new distribution q does not depend on the parameters of p. These parameters are encoded (closed over) in the transformation function T.\n\n\n\n\n\n","category":"function"},{"location":"api/#DifferentiableExpectations.samples-Union{Tuple{threaded}, Tuple{DifferentiableExpectation{threaded}, Vararg{Any}}} where threaded","page":"API reference","title":"DifferentiableExpectations.samples","text":"samples(F::DifferentiableExpectation, Œ∏...; kwargs...)\n\nReturn a vector [f(x‚ÇÅ), ..., f(x‚Çõ)] where the x·µ¢ ‚àº p(Œ∏) are iid samples.\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.mean-Tuple{Any, FixedAtomsProbabilityDistribution}","page":"API reference","title":"Statistics.mean","text":"mean(f, dist::FixedAtomsProbabilityDistribution)\n\nShortcut for mean(map(f, dist)).\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.mean-Union{Tuple{FixedAtomsProbabilityDistribution{threaded}}, Tuple{threaded}} where threaded","page":"API reference","title":"Statistics.mean","text":"mean(dist::FixedAtomsProbabilityDistribution)\n\nCompute the expectation of dist, i.e. the sum of all atoms multiplied by their respective weights.\n\n\n\n\n\n","category":"method"},{"location":"background/#Background","page":"Background","title":"Background","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"Consider a function f mathbbR^n to mathbbR^m and a parametric probability distribution p(theta) on the input space mathbbR^n. Given a random variable X sim p(theta), we want to differentiate the following expectation with respect to theta:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"F(theta) = mathbbE_p(theta)f(X)","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Since F is a vector-to-vector function, the key quantity we want to compute is its Jacobian matrix partial F(theta) in mathbbR^m times n. However, to implement automatic differentiation, we only need vector-Jacobian products (VJPs) partial F(theta)^top v with v in mathbbR^m, also called pullbacks. See the book by Blondel and Roulet (2024) to know more.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Most of the math below is taken from Mohamed et al. (2020).","category":"page"},{"location":"background/#REINFORCE","page":"Background","title":"REINFORCE","text":"","category":"section"},{"location":"background/#Principle","page":"Background","title":"Principle","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"The REINFORCE estimator is derived with the help of the identity nabla log u = nabla u  u:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginaligned\nF(theta + varepsilon)\n = int f(x)  p(x theta + varepsilon)  mathrmdx \n approx int f(x)  left(p(x theta) + nabla_theta p(x theta)^top varepsilonright)  mathrmdx \n = int f(x)  left(p(x theta) + p(x theta) nabla_theta log p(x theta)^top varepsilonright)  mathrmdx \n = F(theta) + left(int f(x)  p(x theta) nabla_theta log p(x theta)^top  mathrmdxright) varepsilon \n = F(theta) + mathbbE_p(theta) leftf(X) nabla_theta log p(X theta)^topright  varepsilon \nendaligned","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"We thus identify the Jacobian matrix:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"partial F(theta) = mathbbE_p(theta) leftf(X) nabla_theta log p(X theta)^topright","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"And the vector-Jacobian product:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"partial F(theta)^top v = mathbbE_p(theta) left(f(X)^top v) nabla_theta log p(X theta)right","category":"page"},{"location":"background/#Variance-reduction","page":"Background","title":"Variance reduction","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"warning: Warning\nWork in progress.","category":"page"},{"location":"background/#Reparametrization","page":"Background","title":"Reparametrization","text":"","category":"section"},{"location":"background/#Trick","page":"Background","title":"Trick","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"The reparametrization trick assumes that we can rewrite the random variable X sim p(theta) as X = g(Z theta), where Z sim q is another random variable whose distribution does not depend on theta.","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"beginaligned\nF(theta + varepsilon)\n = int f(g(z theta + varepsilon))  q(z)  mathrmdz \n approx int fleft(g(z theta) + partial_theta g(z theta)  varepsilonright)  q(z)  mathrmdz \n approx F(theta) + int partial f(g(z theta))  partial_theta g(z theta)  varepsilon  q(z)  mathrmdz \n approx F(theta) + mathbbE_q left partial f(g(Z theta))  partial_theta g(Z theta) right  varepsilon \nendaligned","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"If we denote h(z theta) = f(g(z theta)), then we identify the Jacobian matrix:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"partial F(theta) = mathbbE_q left partial_theta h(Z theta) right","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"And the vector-Jacobian product:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"partial F(theta)^top v = mathbbE_q left partial_theta h(Z theta)^top v right","category":"page"},{"location":"background/#Catalogue","page":"Background","title":"Catalogue","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"The following reparametrizations are implemented:","category":"page"},{"location":"background/","page":"Background","title":"Background","text":"Univariate Normal: X sim mathcalN(mu sigma^2) is equivalent to X = mu + sigma Z with Z sim mathcalN(0 1).\nMultivariate Normal: X sim mathcalN(mu Sigma) is equivalent to X = mu + L Z with Z sim mathcalN(0 I) and L L^top = Sigma. The matrix L can be obtained by Cholesky decomposition of Sigma.","category":"page"},{"location":"background/#Bibliography","page":"Background","title":"Bibliography","text":"","category":"section"},{"location":"background/","page":"Background","title":"Background","text":"Blondel,¬†M. and Roulet,¬†V. (2024). The Elements of Differentiable Programming, arXiv:2403.14606 [cs].\n\n\n\nMohamed,¬†S.; Rosca,¬†M.; Figurnov,¬†M. and Mnih,¬†A. (2020). Monte Carlo Gradient Estimation in Machine Learning. Journal¬†of¬†Machine¬†Learning¬†Research 21, 1‚Äì62.\n\n\n\n","category":"page"},{"location":"#DifferentiableExpectations.jl","page":"Home","title":"DifferentiableExpectations.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Stable) (Image: Dev) (Image: Build Status) (Image: Coverage) (Image: Code Style: Blue)","category":"page"},{"location":"","page":"Home","title":"Home","text":"A Julia package for differentiating through expectations with Monte-Carlo estimates.","category":"page"},{"location":"","page":"Home","title":"Home","text":"It allows the computation of approximate derivatives for functions of the form","category":"page"},{"location":"","page":"Home","title":"Home","text":"F(theta) = mathbbE_p(theta)f(X)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The following estimators are implemented:","category":"page"},{"location":"","page":"Home","title":"Home","text":"REINFORCE\nReparametrization","category":"page"},{"location":"","page":"Home","title":"Home","text":"Warning: this package is experimental, use at your own risk and expect frequent breaking releases.","category":"page"}]
}
